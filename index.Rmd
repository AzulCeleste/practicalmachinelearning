---
title: "Practical Machine Learning Project - Prediction Assignment"
output: html_document
---
## Introduction

In this project, the goal is to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants who performed barbell lifts correctly and incorrectly in 5 different ways to predict the manner in which they did the exercise. 

The training data for this project are available here: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available here: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-tes

Data was taken from the website: http://groupware.les.inf.puc-rio.br/har 

Requirements of the project:
1) describe how you built your model,
2) describe how you used cross validation, 
3) determine the expected out of sample error is, and why you made the choices you did, and 
4) use the prediction model to predict 20 different test cases.

## Loading of data and cleaning
## Introduction

In this project, the goal is to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants who performed barbell lifts correctly and incorrectly in 5 different ways to predict the manner in which they did the exercise. 

The training data for this project are available here: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available here: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-tes

Data was taken from the website: http://groupware.les.inf.puc-rio.br/har 

Requirements of the project:
1) describe how you built your model,
2) describe how you used cross validation, 
3) determine the expected out of sample error is, and why you made the choices you did, and 
4) use the prediction model to predict 20 different test cases.

## Loading of data and cleaning
```{r, echo=FALSE, results='hide'}
setwd("C:/Users/210067772/Documents/GitHub/practicalmachinelearning")
```
```{r, echo=TRUE, results='hide', message=FALSE, warning=FALSE}
library(caret)
library(rattle)
library(randomForest)
```
```{r, echo=TRUE, warning=FALSE}
#Consider empty values as NA
training <- read.csv("pml-training.csv", na.strings = c("NA","#DIV/0!",""), header = TRUE)
testing <- read.csv("pml-testing.csv", na.strings = c("NA","#DIV/0!",""), header = TRUE)

#Separate the variables related to the accelerometers on the belt, forearm, arm, and dumbbell
names  <-  colnames(training)
dataLogical <- (grepl("_belt",names) | grepl("_forearm",names) | grepl("_arm",names)| grepl("_dumbbell", names) | grepl("classe", names))
accelData <- training[dataLogical==TRUE]
names(accelData); dim(accelData) # 19622   153
```
```{r, echo=FALSE, results='hide'}
#To check if the resulting data frame has no missing values with:
table(complete.cases(accelData))
```
There are a lot of NA values in the data, let's remove all columns with NAs more than 80% of the column data
```{r, echo=TRUE, results='hide'}
NAdata = is.na(accelData)
noColumns = which(colSums(NAdata) > 13700)
accelData = accelData[, -noColumns]
dim(accelData) # 19622 53
```
## Analyzing data

```{r, echo=TRUE, results='hide'}
#Creating a partition
set.seed(32343)
inTrain <- createDataPartition(accelData$classe, p=0.6, list=FALSE)
myTrain <- accelData[inTrain, ]
myTest <- accelData[-inTrain, ]
dim(myTrain); dim(myTest) # 11776 53 | 7846 53

#Analyzing the presence of NZV
nsv <- nearZeroVar(myTrain, saveMetrics=TRUE)
nsv # All predictors are FALSE, no covariates
```
The first method used was *rpart*. The accuracy was very low, 54.8%, then I discarded it.
```{r, echo=TRUE, warning=FALSE}
modelFit <- train(classe~.,data=myTrain, method="rpart")
modelFit
predictions <- predict(modelFit,newdata=myTest)
confusionMatrix(predictions,myTest$classe)
```
I read in the discussion forums that *random forest* method was very time consumming but effective then I decided to try it. In my first attempt, I used the *train* function but it took so long so I followed the recommendation from forums to use the *randomForest* function directly, it only took a couple of minutes: 
```{r, echo=TRUE}
modFit <- randomForest(classe ~ ., data = myTrain)
predictionsRF <- predict(modFit,newdata=myTest)
confusionMatrix(predictionsRF,myTest$classe)
```
The accuracy of this model in the test data is very good, 99.41%. To cross validate the model we build a new training/test sets and repeat the model prediction.

```{r, echo=TRUE}
inTrain <- createDataPartition(accelData$classe, p=0.75, list=FALSE)
myTrain2 <- accelData[inTrain, ]
myTest2 <- accelData[-inTrain, ]
modFit2 <- randomForest(classe ~ ., data = myTrain2)
predictionsRF2 <- predict(modFit,newdata=myTest2)
confusionMatrix(predictionsRF2,myTest2$classe)

```
The accuracy of this model is higher than the first one, 99.69%, however, we decided to consider *the out of sample error* of the model from the first one, 100-99.41 = 0.59%,  since it is more conservative.

Finally, we predicted the values for the testing data with both prediction models:
```{r, echo=TRUE}
predict(modFit, newdata=testing)
predict(modFit2, newdata=testing)
```
The predicted values are equal with both models. We introduced the data in the quiz and I got a 20/20 confirming the accuracy of the model was good.

```{r, echo=TRUE, results='hide', message=FALSE, warning=FALSE}
library(caret)
library(rattle)
library(randomForest)
```
```{r, echo=TRUE, warning=FALSE}
#Consider empty values as NA
training <- read.csv("pml-training.csv", na.strings = c("NA","#DIV/0!",""), header = TRUE)
testing <- read.csv("pml-testing.csv", na.strings = c("NA","#DIV/0!",""), header = TRUE)

#Separate the variables related to the accelerometers on the belt, forearm, arm, and dumbbell
names  <-  colnames(training)
dataLogical <- (grepl("_belt",names) | grepl("_forearm",names) | grepl("_arm",names)| grepl("_dumbbell", names) | grepl("classe", names))
accelData <- training[dataLogical==TRUE]
names(accelData); dim(accelData) # 19622   153
```
```{r, echo=FALSE, results='hide'}
#To check if the resulting data frame has no missing values with:
table(complete.cases(accelData))
```
There are a lot of NA values in the data, let's remove all columns with NAs more than 80% of the column data
```{r, echo=TRUE, results='hide'}
NAdata = is.na(accelData)
noColumns = which(colSums(NAdata) > 13700)
accelData = accelData[, -noColumns]
dim(accelData) # 19622 53
```
## Analyzing data

```{r, echo=TRUE, results='hide'}
#Creating a partition
set.seed(32343)
inTrain <- createDataPartition(accelData$classe, p=0.6, list=FALSE)
myTrain <- accelData[inTrain, ]
myTest <- accelData[-inTrain, ]
dim(myTrain); dim(myTest) # 11776 53 | 7846 53

#Analyzing the presence of NZV
nsv <- nearZeroVar(myTrain, saveMetrics=TRUE)
nsv # All predictors are FALSE, no covariates
```
The first method used was *rpart*. The accuracy was very low, 54.8%, then I discarded it.
```{r, echo=TRUE, warning=FALSE}
modelFit <- train(classe~.,data=myTrain, method="rpart")
modelFit
predictions <- predict(modelFit,newdata=myTest)
confusionMatrix(predictions,myTest$classe)
```
I read in the discussion forums that *random forest* method was very time consumming but effective then I decided to try it. In my first attempt, I used the *train* function but it took so long so I followed the recommendation from forums to use the *randomForest* function directly, it only took a couple of minutes: 
```{r, echo=TRUE}
modFit <- randomForest(classe ~ ., data = myTrain)
predictionsRF <- predict(modFit,newdata=myTest)
confusionMatrix(predictionsRF,myTest$classe)
```
The accuracy of this model in the test data is very good, 99.41%. To cross validate the model we build a new training/test sets and repeat the model prediction.

```{r, echo=TRUE}
inTrain <- createDataPartition(accelData$classe, p=0.75, list=FALSE)
myTrain2 <- accelData[inTrain, ]
myTest2 <- accelData[-inTrain, ]
modFit2 <- randomForest(classe ~ ., data = myTrain2)
predictionsRF2 <- predict(modFit,newdata=myTest2)
confusionMatrix(predictionsRF2,myTest2$classe)

```
The accuracy of this model is higher than the first one, 99.69%, however, we decided to consider *the out of sample error* of the model from the first one, 100-99.41 = 0.59%,  since it is more conservative.

Finally, we predicted the values for the testing data with both prediction models:
```{r, echo=TRUE}
predict(modFit, newdata=testing)
predict(modFit2, newdata=testing)
```
The predicted values are equal with both models. We introduced the data in the quiz and I got a 20/20 confirming the accuracy of the model was good.
